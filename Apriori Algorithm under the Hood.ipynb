{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_3_itemsets(base_itemsets):\n",
    "    candidates = []\n",
    "    full_candidates = []\n",
    "    candidate_items = set()\n",
    "    for itemset in base_itemsets:\n",
    "        for i in itemset:\n",
    "            candidate_items.add(i)\n",
    "    \n",
    "    for i in base_itemsets:\n",
    "        for k in candidate_items:\n",
    "            if k not in i:\n",
    "                x = set()\n",
    "                x.update(i)\n",
    "                x.update(k)\n",
    "                if x not in full_candidates:\n",
    "                    full_candidates.append(x)\n",
    "\n",
    "    for x in full_candidates:\n",
    "        a,b,c = list(x)\n",
    "        if set([a,b]) in base_itemsets and set([a,c]) in base_itemsets and set([b,c]) in base_itemsets:\n",
    "            candidates.append(x)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Apriori/food_emoji_frequent_2_itemsets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_2_itemsets = []\n",
    "with open('../Apriori/food_emoji_frequent_2_itemsets.csv', 'r', encoding=\"utf8\") as fin:\n",
    "    for line in fin:\n",
    "        frequent_2_itemsets.append(set([line[0], line[1]]))\n",
    "\n",
    "candidate_3_itemsets = generate_candidate_3_itemsets(frequent_2_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"../Apriori/food_drink_emoji_tweets.txt\", sep=\"\\t\", header=None)\n",
    "tweets_df.columns = ['text']\n",
    "\n",
    "emoji_list = \"ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸ¥­ğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ¥ğŸ…ğŸ¥¥ğŸ¥‘ğŸ†ğŸ¥”ğŸ¥•ğŸŒ½ğŸŒ¶ğŸ¥’ğŸ¥¬ğŸ¥¦ğŸ„ğŸ¥œğŸŒ°ğŸğŸ¥ğŸ¥–ğŸ¥¨ğŸ¥¯ğŸ¥ğŸ§€ğŸ–ğŸ—ğŸ¥©ğŸ¥“ğŸ”ğŸŸğŸ•ğŸŒ­ğŸ¥ªğŸŒ®ğŸŒ¯ğŸ¥™ğŸ¥šğŸ³ğŸ¥˜ğŸ²ğŸ¥£ğŸ¥—ğŸ¿ğŸ§‚ğŸ¥«ğŸ±ğŸ˜ğŸ™ğŸšğŸ›ğŸœğŸğŸ ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¥®ğŸ¡ğŸ¥ŸğŸ¥ ğŸ¥¡ğŸ¦€ğŸ¦ğŸ¦ğŸ¦‘ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ‚ğŸ°ğŸ§ğŸ¥§ğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ¼ğŸ¥›â˜•ğŸµğŸ¶ğŸ¾ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¥‚ğŸ¥ƒ\"\n",
    "emoji_set = set(emoji_list)\n",
    "\n",
    "def extract_uniq_emojis(text):\n",
    "    return \n",
    "\n",
    "tweets_df['emojis'] = tweets_df.text.apply(lambda text:np.unique([chr for chr in text if chr in emoji_set]))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "emoji_matrix = pd.DataFrame(data=mlb.fit_transform(tweets_df.emojis), index=tweets_df.index, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequent_itemsets(candidate_itemsets, min_support=0.005):\n",
    "    frequent_itemsets = []\n",
    "    for i in candidate_itemsets:\n",
    "        x = 0\n",
    "        for k in tweets_df['emojis']:\n",
    "            if i.issubset(set(k)):            \n",
    "                x = x+1\n",
    "        y = x/len(tweets_df['emojis'])\n",
    "        if y >= min_support:\n",
    "            frequent_itemsets.append(i)\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ğŸ‡', 'ğŸ‰', 'ğŸ¥'},\n",
       " {'ğŸ‡', 'ğŸ‰', 'ğŸŠ'},\n",
       " {'ğŸ‡', 'ğŸ‰', 'ğŸ'},\n",
       " {'ğŸ‡', 'ğŸŠ', 'ğŸ¥'},\n",
       " {'ğŸ‡', 'ğŸŠ', 'ğŸ'},\n",
       " {'ğŸ‡', 'ğŸ', 'ğŸ¥'},\n",
       " {'ğŸ‰', 'ğŸŠ', 'ğŸ¥'},\n",
       " {'ğŸ‰', 'ğŸŠ', 'ğŸ'},\n",
       " {'ğŸ‰', 'ğŸ', 'ğŸ¥'},\n",
       " {'ğŸŠ', 'ğŸ', 'ğŸ¥'},\n",
       " {'ğŸ”', 'ğŸ•', 'ğŸŸ'},\n",
       " {'ğŸ¦', 'ğŸ§', 'ğŸ¨'},\n",
       " {'ğŸ«', 'ğŸ¬', 'ğŸ­'},\n",
       " {'ğŸ«', 'ğŸ°', 'ğŸ‚'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_frequent_itemsets(candidate_3_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
